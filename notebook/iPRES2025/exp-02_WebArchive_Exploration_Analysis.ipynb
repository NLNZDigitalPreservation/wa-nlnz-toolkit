{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5520611",
   "metadata": {},
   "source": [
    "# Exploring the NLNZ Web Archive Dataset\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to explore and analyze web archive data from the National Library of New Zealand (NLNZ). It builds upon the access methods covered in the previous notebook and focuses on analyzing archived web content.\n",
    "\n",
    "### Target Website\n",
    "- **Website:** covid19.govt.nz (historical site, no longer active)\n",
    "- **Dataset:** covid19.govt.nz crawls (2020â€“2023, WARC + CDX, HTML only)\n",
    "\n",
    "### Analysis Sections\n",
    "1. **Temporal Coverage Analysis** - Examining capture frequency over time\n",
    "2. **Content Size Evolution** - Tracking how the size of captured HTML pages changed\n",
    "3. **Structural/URL Analysis** - Analyzing URL patterns and website structure evolution\n",
    "4. **Text Content Exploration** - Extracting and analyzing textual content from archives\n",
    "\n",
    "> **NOTE:** This notebook builds on the same Python packages used in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc834a",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "### Installing Required Python Packages\n",
    "\n",
    "The following packages are required for web archive analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies for web archive processing\n",
    "!pip -q install warcio>=1.7.4 validators boto3>=1.40.26 s3fs bs4 wordcloud\n",
    "\n",
    "# Install packages for webpage screenshots (optional visualization)\n",
    "!pip -q install selenium chromedriver-autoinstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9358f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the NLNZ Web Archive Toolkit\n",
    "!pip -q install -i https://test.pypi.org/simple/ wa-nlnz-toolkit==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187099d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment based on execution context\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Set output directory based on execution environment\n",
    "res_folder = \"/content\" if IN_COLAB else \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import wa_nlnz_toolkit as want  # NLNZ Web Archive Toolkit\n",
    "import pandas as pd             # Data manipulation\n",
    "import os                       # File operations\n",
    "import numpy as np              # Numerical operations\n",
    "from tqdm import tqdm           # Progress bars\n",
    "from collections import Counter # For word frequency analysis\n",
    "\n",
    "# Define target website\n",
    "webpage = \"covid19.govt.nz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c5f14",
   "metadata": {},
   "source": [
    "## 1. Temporal Coverage Analysis\n",
    "\n",
    "This section analyzes how frequently the website was captured over time. We'll use the CDX API to query captures of the homepage and analyze the temporal distribution of these captures.\n",
    "\n",
    "### Querying the CDX Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the CDX index for all captures of the target website\n",
    "df_captures = want.query_cdx_index(webpage)\n",
    "\n",
    "# Filter for successful captures only (HTTP status code 200)\n",
    "df_captures = df_captures[df_captures[\"status\"] == \"200\"]\n",
    "\n",
    "# Display information about the first capture\n",
    "df_captures.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24249092",
   "metadata": {},
   "source": [
    "### Temporal Distribution Analysis\n",
    "\n",
    "The data shows that the first capture of covid19.govt.nz was on March 18, 2020, coinciding with the early stages of the COVID-19 pandemic. The capture frequency peaked in April 2020 with over 60 captures, followed by a slight decrease in May 2020.\n",
    "\n",
    "After this initial surge, the crawl frequency decreased significantly but maintained a relatively stable level of approximately 10-20 crawls per month until mid-2023. In late 2023, the frequency further decreased to about 5 crawls per month, likely reflecting the reduced importance of the site as the pandemic situation evolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd2c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the monthly capture frequency\n",
    "want.plot_monthly_captures(df_captures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display access URLs for all captures\n",
    "df_captures[\"access_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0353d",
   "metadata": {},
   "source": [
    "### (Optional) Visual Comparison with Screenshots\n",
    "\n",
    "To better understand how the website evolved over time, we can capture screenshots of the archived versions from different time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce463c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Chromium browser if running in Google Colab\n",
    "if IN_COLAB:\n",
    "    !apt-get update\n",
    "    !apt-get install -y chromium-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208090e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture screenshots of the earliest and latest archived versions\n",
    "# First capture (March 2020)\n",
    "want.screenshot_webpage(\"https://ndhadeliver.natlib.govt.nz/webarchive/20200318051641/https://covid19.govt.nz/\", \n",
    "                        os.path.join(res_folder, \"covid19_20200318051641.png\"))\n",
    "\n",
    "# Recent capture (February 2024)\n",
    "want.screenshot_webpage(\"https://ndhadeliver.natlib.govt.nz/webarchive/20240211071903/https://covid19.govt.nz/\", \n",
    "                        os.path.join(res_folder, \"covid19_20250827233627.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43f616",
   "metadata": {},
   "source": [
    "## 2. Content Size Evolution\n",
    "\n",
    "This section tracks how the size of the website's content changed over time. We'll analyze the pre-processed web archive data that contains captured HTML pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 bucket and folder containing the archive data\n",
    "bucket_name = \"ndha-public-data-ap-southeast-2\"\n",
    "folder_prefix = \"iPRES-2025/sample-data/covid19.govt.nz/\"\n",
    "\n",
    "# List all files in the S3 bucket folder\n",
    "all_files = want.list_s3_files(bucket_name, folder_prefix)\n",
    "\n",
    "# Filter for CDX index files\n",
    "cdx_files = [f for f in all_files if f.endswith(\".cdx\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the total number of crawls in the dataset\n",
    "print(f\"In total, there are {len(cdx_files)} crawls in the sample dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e26211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datetime information from CDX filenames\n",
    "list_dt = [cdx_file.split(\"/\")[-2].split(\"_\")[0] for cdx_file in cdx_files]\n",
    "\n",
    "# Calculate total content size for each crawl\n",
    "list_size = []\n",
    "for cdx_file in cdx_files:\n",
    "    # Load CDX file data\n",
    "    df_cdx = want.load_cdx_file_from_s3(bucket_name, cdx_file)\n",
    "    \n",
    "    # Filter for valid HTML responses (status 200, MIME type text/html, no JSON fragments)\n",
    "    df_cdx_valid_html = df_cdx[(df_cdx[\"m\"] == \"text/html\") \n",
    "                                & (df_cdx[\"s\"] == 200)\n",
    "                                & (~df_cdx[\"a\"].str.contains(\"%22\"))]\n",
    "\n",
    "    # Calculate total size in MB and append to list\n",
    "    list_size.append(df_cdx_valid_html[\"S\"].sum() / 1024 / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with datetime and size information\n",
    "df_size = pd.DataFrame({\"dt\": pd.to_datetime(list_dt), \"compressed payload size\": list_size})\n",
    "df_size.set_index(\"dt\", inplace=True)\n",
    "df_size.sort_index(inplace=True)\n",
    "\n",
    "# Visualize content size evolution over time\n",
    "df_size.plot(figsize=(12, 6), \n",
    "             title=\"Compressed Payload Size Evolution Over Time\", \n",
    "             ylabel=\"Total Size (MB)\", \n",
    "             xlabel=\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc474654",
   "metadata": {},
   "source": [
    "## 3. Structural/URL Analysis\n",
    "\n",
    "This section explores how the website's structure evolved over time by analyzing URL patterns and diversity.\n",
    "\n",
    "### Analysis Focus Areas\n",
    "- URL diversity and growth over time\n",
    "- Identification of core vs. peripheral URLs\n",
    "- URL path structure analysis\n",
    "- URL lifecycle (creation and removal of pages)\n",
    "- Snapshot comparison across time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique URLs for each crawl\n",
    "list_unique_urls = []\n",
    "for cdx_file in cdx_files:\n",
    "    # Load CDX file data\n",
    "    df_cdx = want.load_cdx_file_from_s3(bucket_name, cdx_file)\n",
    "    \n",
    "    # Filter for valid HTML responses\n",
    "    df_cdx_valid_html = df_cdx[(df_cdx[\"m\"] == \"text/html\") \n",
    "                                & (df_cdx[\"s\"] == 200) \n",
    "                                & (~df_cdx[\"a\"].str.contains(\"%22\"))]\n",
    "\n",
    "    # Count unique URLs in this crawl\n",
    "    list_unique_urls.append(len(df_cdx_valid_html[\"a\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09711087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique URL counts to the size DataFrame\n",
    "df_size[\"unique urls\"] = list_unique_urls\n",
    "\n",
    "# Visualize URL count evolution over time\n",
    "df_size[\"unique urls\"].plot(figsize=(12, 6), \n",
    "                            title=\"Evolution of Unique URLs Over Time\", \n",
    "                            ylabel=\"Number of Unique URLs\", \n",
    "                            xlabel=\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee54e486",
   "metadata": {},
   "source": [
    "### Core vs. Peripheral URLs Analysis\n",
    "\n",
    "To identify the most important pages on the website, we'll analyze which URLs appear most frequently across all crawls. This helps distinguish between core content (consistently present) and peripheral content (temporary or less important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all unique URLs across all crawls\n",
    "unique_urls = np.array([])\n",
    "for cdx_file in cdx_files[:]:\n",
    "    # Load CDX file data\n",
    "    df_cdx = want.load_cdx_file_from_s3(bucket_name, cdx_file)\n",
    "    \n",
    "    # Filter for valid HTML responses\n",
    "    df_cdx_valid_html = df_cdx[(df_cdx[\"m\"] == \"text/html\") \n",
    "                                & (df_cdx[\"s\"] == 200) \n",
    "                                & (~df_cdx[\"a\"].str.contains(\"%22\"))]\n",
    "\n",
    "    # Append unique URLs from this crawl\n",
    "    unique_urls = np.append(unique_urls, df_cdx_valid_html[\"a\"].unique())\n",
    "\n",
    "# Create a DataFrame with all collected URLs\n",
    "df_unique_urls = pd.DataFrame(unique_urls, columns=[\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813205dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export URL frequency counts to CSV\n",
    "df_unique_urls.value_counts().to_csv(os.path.join(res_folder, \"url_counts.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da19e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze URL structure by extracting first-level path components\n",
    "# Remove duplicates and query parameters\n",
    "df_unique_urls = df_unique_urls.drop_duplicates(subset=[\"url\"])\n",
    "df_unique_urls = df_unique_urls[~df_unique_urls[\"url\"].str.contains(\"?\", regex=False)]\n",
    "\n",
    "# Extract first-level path component (e.g., \"about\" from \"covid19.govt.nz/about/...\")\n",
    "df_unique_urls[\"level_1_subdomain\"] = df_unique_urls[\"url\"].apply(lambda x: x.split(\"/\")[3])\n",
    "\n",
    "# Display top 10 first-level path components\n",
    "df_unique_urls[\"level_1_subdomain\"].value_counts().head(10)\n",
    "\n",
    "# Export all first-level path components to CSV\n",
    "df_unique_urls[\"level_1_subdomain\"].value_counts().to_csv(os.path.join(res_folder, \"level_1_subdomains.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d7299",
   "metadata": {},
   "source": [
    "### URL Lifecycle Analysis Exercise\n",
    "\n",
    "The following exercise demonstrates how to track when a specific URL first appeared and when it was last seen in the archive. This helps understand the lifecycle of website sections.\n",
    "\n",
    "> **Hands-on Exercise:** Complete the code below to find when a specific URL (e.g., \"https://covid19.govt.nz/traffic-lights/\") first appeared in the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18431819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL to search for\n",
    "url_to_be_found = \"https://covid19.govt.nz/traffic-lights/\"\n",
    "\n",
    "# Search through all CDX files for this URL\n",
    "for cdx_file in cdx_files:\n",
    "    # Load CDX file data\n",
    "    df_cdx = want.load_cdx_file_from_s3(bucket_name, cdx_file)\n",
    "    \n",
    "    # Filter for valid HTML responses\n",
    "    df_cdx_valid_html = df_cdx[(df_cdx[\"m\"] == \"text/html\") \n",
    "                                & (df_cdx[\"s\"] == 200) \n",
    "                                & (~df_cdx[\"a\"].str.contains(\"%22\"))]\n",
    "\n",
    "    # SOLUTION (commented out)\n",
    "    # # Filter for the target URL\n",
    "    # df_cdx_valid_html = df_cdx_valid_html[df_cdx_valid_html[\"a\"] == url_to_be_found]\n",
    "    # if not df_cdx_valid_html.empty:\n",
    "    #     # Print the timestamp when this URL was found\n",
    "    #     print(df_cdx_valid_html[\"b\"].tolist()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abac831",
   "metadata": {},
   "source": [
    "## 4. Text Content Exploration\n",
    "\n",
    "This section focuses on extracting and analyzing the textual content from archived HTML pages. This allows us to track how the website's messaging evolved over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f6370",
   "metadata": {},
   "source": [
    "### Setting Up Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 bucket and folder containing the archive data\n",
    "bucket_name = \"ndha-public-data-ap-southeast-2\"\n",
    "folder_prefix = \"iPRES-2025/sample-data/covid19.govt.nz/\"\n",
    "\n",
    "# List all files in the S3 bucket folder\n",
    "all_files = want.list_s3_files(bucket_name, folder_prefix)\n",
    "\n",
    "# Helper function to locate a specific WARC file in the S3 bucket\n",
    "def find_warc_file_path(warc_file):\n",
    "    \"\"\"Find the full S3 path for a given WARC filename.\n",
    "    \n",
    "    Args:\n",
    "        warc_file (str): The WARC filename to search for\n",
    "        \n",
    "    Returns:\n",
    "        str: Full S3 path if found, None otherwise\n",
    "    \"\"\"\n",
    "    for s3_file in all_files:\n",
    "        if warc_file in s3_file:\n",
    "            warc_file_path = \"s3://ndha-public-data-ap-southeast-2/\" + s3_file\n",
    "            return warc_file_path\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45474d26",
   "metadata": {},
   "source": [
    "### Single Crawl Text Extraction\n",
    "\n",
    "First, we'll extract text content from a single CDX file to demonstrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first CDX file for demonstration\n",
    "cdf_file = cdx_files[0]\n",
    "\n",
    "# Load CDX file data\n",
    "df_cdx = want.load_cdx_file_from_s3(bucket_name, cdf_file)\n",
    "\n",
    "# Extract date from filename\n",
    "dt = cdf_file.split(\"/\")[-2].split(\"_\")[0]\n",
    "\n",
    "# Initialize lists to store content and URLs\n",
    "content_all = []\n",
    "url_all = []\n",
    "\n",
    "# Process each entry in the CDX file\n",
    "for idx in tqdm(range(len(df_cdx)), desc=f\"Extracting contents for {dt}\"):\n",
    "    # Get WARC file and offset information\n",
    "    warc_file = df_cdx.iloc[idx][\"g\"]\n",
    "    offset = int(df_cdx.iloc[idx][\"V\"])\n",
    "\n",
    "    # Extract HTML payload and text content\n",
    "    html_payload = want.extract_payload(find_warc_file_path(warc_file), offset)\n",
    "    content = want.extract_content_html(html_payload)\n",
    "    content_all.append(content)\n",
    "    \n",
    "    # Construct access URL for this capture\n",
    "    url = \"https://ndhadeliver.natlib.govt.nz/webarchive/{}/{}\".format(df_cdx.iloc[idx][\"b\"], df_cdx.iloc[idx][\"a\"])\n",
    "    url_all.append(url)\n",
    "\n",
    "# Remove duplicate content\n",
    "content_cleaned = []\n",
    "url_cleaned = []\n",
    "for content, url in zip(content_all, url_all):\n",
    "    content_joined = \" \".join(content)\n",
    "    if content_joined not in content_cleaned:\n",
    "        content_cleaned.append(content_joined)\n",
    "        url_cleaned.append(url)\n",
    "\n",
    "# Save cleaned content to text files\n",
    "with open(os.path.join(res_folder, f\"covid19_content_cleaned_{dt}.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(content_cleaned))\n",
    "    \n",
    "with open(os.path.join(res_folder, f\"covid19_url_cleaned_{dt}.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(url_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f010b",
   "metadata": {},
   "source": [
    "### Batch Processing All Crawls\n",
    "\n",
    "Now we'll extend the text extraction process to all crawls in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for storing extracted content\n",
    "content_dir = os.path.join(res_folder, \"covid19_corpus/raw/content\")\n",
    "url_dir = os.path.join(res_folder, \"covid19_corpus/raw/url\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(content_dir, exist_ok=True)\n",
    "os.makedirs(url_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74892c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each CDX file\n",
    "for cdx_file in cdx_files:\n",
    "    # Load CDX file data\n",
    "    df_cdx = want.load_cdx_file_from_s3(bucket_name, cdx_file)\n",
    "    \n",
    "    # Extract date from filename\n",
    "    dt = cdx_file.split(\"/\")[-2].split(\"_\")[0]\n",
    "\n",
    "    # Initialize lists to store content and URLs\n",
    "    content_all = []\n",
    "    url_all = []\n",
    "    \n",
    "    # Process each entry in the CDX file\n",
    "    for idx in tqdm(range(len(df_cdx)), desc=f\"Extracting contents for {dt}\"):\n",
    "        # Get WARC file and offset information\n",
    "        warc_file = df_cdx.iloc[idx][\"g\"]\n",
    "        offset = int(df_cdx.iloc[idx][\"V\"])\n",
    "\n",
    "        # Extract HTML payload and text content\n",
    "        html_payload = want.extract_payload(find_warc_file_path(warc_file), offset)\n",
    "        content = want.extract_content_html(html_payload)\n",
    "        content_all.append(content)\n",
    "        \n",
    "        # Construct access URL for this capture\n",
    "        url = \"https://ndhadeliver.natlib.govt.nz/webarchive/{}/{}\".format(df_cdx.iloc[idx][\"b\"], df_cdx.iloc[idx][\"a\"])\n",
    "        url_all.append(url)\n",
    "\n",
    "        # Log URLs with no content extracted\n",
    "        if content == []:\n",
    "            print(f\"No content extracted from: {url}\")\n",
    "\n",
    "    # Remove duplicate content\n",
    "    content_cleaned = []\n",
    "    url_cleaned = []\n",
    "    for content, url in zip(content_all, url_all):\n",
    "        content_joined = \" \".join(content)\n",
    "        if content_joined not in content_cleaned:\n",
    "            content_cleaned.append(content_joined)\n",
    "            url_cleaned.append(url)\n",
    "\n",
    "    # Save cleaned content to text files\n",
    "    with open(os.path.join(content_dir, f\"covid19_content_cleaned_{dt}.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(content_cleaned))\n",
    "        \n",
    "    with open(os.path.join(url_dir, f\"covid19_url_cleaned_{dt}.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(url_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65587ca8",
   "metadata": {},
   "source": [
    "### Text Analysis\n",
    "\n",
    "Now that we have extracted text content, we can perform various analyses to understand the website's messaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word frequency in the extracted content\n",
    "word_freq = Counter(\" \".join(content_cleaned).split())\n",
    "\n",
    "# Display the 10 most common words\n",
    "word_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bebe6f",
   "metadata": {},
   "source": [
    "### Visualization with Word Clouds\n",
    "\n",
    "Word clouds provide a visual representation of the most frequent terms in the text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud from the cleaned text content\n",
    "want.create_world_cloud(content_cleaned, os.path.join(res_folder, \"covid19_wordcloud.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc364257",
   "metadata": {},
   "source": [
    "### Temporal Content Analysis\n",
    "\n",
    "Finally, we'll track how the website's content evolved over time by generating word clouds for the homepage at different points in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL to analyze (homepage)\n",
    "url_to_be_analysed = \"https://covid19.govt.nz/\"\n",
    "\n",
    "# Process each CDX file\n",
    "for i, cdx_file in enumerate(cdx_files):\n",
    "    # Load CDX file data\n",
    "    df_cdx = want.load_cdx_file_from_s3(bucket_name, cdx_file)\n",
    "    \n",
    "    # Filter for valid HTML responses\n",
    "    df_cdx_valid_html = df_cdx[(df_cdx[\"m\"] == \"text/html\") \n",
    "                                & (df_cdx[\"s\"] == 200)\n",
    "                                & (~df_cdx[\"a\"].str.contains(\"%22\"))\n",
    "                                ]\n",
    "\n",
    "    # Filter for the homepage URL\n",
    "    df_cdx_valid_html = df_cdx_valid_html[df_cdx_valid_html[\"a\"] == url_to_be_analysed]\n",
    "    \n",
    "    # If the homepage was found in this crawl\n",
    "    if not df_cdx_valid_html.empty:\n",
    "        # Get WARC file and offset information\n",
    "        warc_file = df_cdx_valid_html.iloc[0][\"g\"]\n",
    "        warc_offset = df_cdx_valid_html.iloc[0][\"V\"]\n",
    "\n",
    "        # Extract HTML payload and text content\n",
    "        html_payload = want.extract_payload(find_warc_file_path(warc_file), warc_offset)\n",
    "        content = want.extract_content_html(html_payload)\n",
    "\n",
    "        # Extract date from filename and generate word cloud\n",
    "        dt = cdx_file.split(\"/\")[-2].split(\"_\")[0]\n",
    "        want.create_world_cloud(content, os.path.join(res_folder, f\"wordcloud_{dt}.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "This notebook has demonstrated various techniques for exploring and analyzing web archive data from the NLNZ collection, focusing on the covid19.govt.nz website. We've covered:\n",
    "\n",
    "1. **Temporal analysis** - Understanding when and how frequently the website was captured\n",
    "2. **Content size evolution** - Tracking how the website's size changed over time\n",
    "3. **URL structure analysis** - Examining the website's organization and key sections\n",
    "4. **Text content extraction and analysis** - Exploring the website's messaging through text analysis\n",
    "\n",
    "### Potential Extensions\n",
    "\n",
    "- **Sentiment analysis** of extracted text to track public messaging tone over time\n",
    "- **Topic modeling** to identify key themes and how they evolved\n",
    "- **Network analysis** of internal links to understand site structure\n",
    "- **Image analysis** of extracted visual content\n",
    "- **Comparative analysis** with other COVID-19 information websites\n",
    "\n",
    "These techniques can be applied to other web archives to gain insights into how websites and their content evolve over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
