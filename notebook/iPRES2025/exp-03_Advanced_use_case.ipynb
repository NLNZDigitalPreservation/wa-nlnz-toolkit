{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c782d34d",
   "metadata": {},
   "source": [
    "# Processing Web Archive Text Corpus\n",
    "\n",
    "This notebook demonstrates how to work with text files extracted from web archive HTML snapshots. We'll cover the following steps:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Extracting text from web archive HTML\n",
    "3. Cleaning and preprocessing the text data\n",
    "4. Basic text analysis\n",
    "5. Advanced analysis with embeddings and semantic search\n",
    "6. Building a question-answering system with the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a36034",
   "metadata": {},
   "source": [
    "## 1. Setting up the environment\n",
    "\n",
    "First, let's install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b53589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pre-requisites\n",
    "!pip -q install warcio>=1.7.4 validators boto3>=1.40.26 s3fs bs4 wordcloud\n",
    "!pip -q install selenium chromedriver-autoinstaller\n",
    "!pip -q install sentence-transformers chromadb # additional packages for embeddings and semantic search\n",
    "\n",
    "# Install wa_nlnz_toolkit\n",
    "!pip -q install -i https://test.pypi.org/simple/ wa-nlnz-toolkit==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Set default result folder\n",
    "if IN_COLAB:\n",
    "    res_folder = \"/content/sample_data\"\n",
    "else:\n",
    "    res_folder = \"./sample_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b856e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wa_nlnz_toolkit as want\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd67cfa",
   "metadata": {},
   "source": [
    "## 2. Extracting text from web archive HTML\n",
    "\n",
    "First, let's remind ourselves how to extract text content from web archive HTML files. We'll use the `extract_payload` and `extract_content_html` functions from the `wa_nlnz_toolkit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract HTML payload from a WARC file\n",
    "# Replace with your own WARC file path and offset\n",
    "warc_file = \"s3://ndha-public-data-ap-southeast-2/iPRES-2025/sample-data/covid19.govt.nz/2023-12-14_IE89493927/FL89493929_NLNZ-20231212233435565-00000-72544~wlgprdwctweb01.natlib.govt.nz~8443.warc.gz\"\n",
    "offset = 3126252\n",
    "\n",
    "# Extract the HTML payload\n",
    "html_payload = want.extract_payload(warc_file, offset)\n",
    "\n",
    "# Check if we got a payload\n",
    "if html_payload:\n",
    "    print(f\"Successfully extracted HTML payload of {len(html_payload)} bytes\")\n",
    "else:\n",
    "    print(\"Failed to extract HTML payload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text content from the HTML payload\n",
    "if html_payload:\n",
    "    # Use the toolkit's function to extract content\n",
    "    paragraphs = want.extract_content_html(html_payload)\n",
    "    \n",
    "    # Print the first few paragraphs\n",
    "    print(f\"Extracted {len(paragraphs)} paragraphs. Here are the first 5:\")\n",
    "    for i, para in enumerate(paragraphs[:5]):\n",
    "        print(f\"\\n{i+1}. {para}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225945f9",
   "metadata": {},
   "source": [
    "## 3. Cleaning and preprocessing the text data\n",
    "\n",
    "Now, let's look at how to clean and preprocess the text data from multiple web archive snapshots. We'll work with the existing corpus files in the `covid19_corpus/raw/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc69663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "raw_folder_path = os.path.join(res_folder, \"covid19_corpus/raw/\")\n",
    "cleaned_folder_path = os.path.join(res_folder, \"covid19_corpus/cleaned/\")\n",
    "\n",
    "# Create the cleaned directory if it doesn't exist\n",
    "cleaned_content_dir = os.path.join(cleaned_folder_path, \"content\")\n",
    "cleaned_url_dir = os.path.join(cleaned_folder_path, \"url\")\n",
    "os.makedirs(cleaned_content_dir, exist_ok=True)\n",
    "os.makedirs(cleaned_url_dir, exist_ok=True)\n",
    "\n",
    "# List all text files in the raw folder\n",
    "raw_content_dir = os.path.join(raw_folder_path, \"content\")\n",
    "raw_url_dir = os.path.join(raw_folder_path, \"url\")\n",
    "raw_content_files = [f for f in os.listdir(raw_content_dir) if f.endswith(\".txt\")]\n",
    "print(f\"Found {len(raw_content_files)} raw text files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ff77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a simple function to extract date from filename\n",
    "def extract_date(fname):\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', fname)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "# Find all content files, sort by date (assuming date in filename)\n",
    "content_files = sorted(glob(os.path.join(raw_content_dir, \"covid19_content_cleaned_*.txt\")))\n",
    "seen_contents = set()\n",
    "\n",
    "list_date_tag = []\n",
    "list_unique_pages = []\n",
    "for content_file in content_files:\n",
    "    date_tag = extract_date(os.path.basename(content_file))\n",
    "    url_file = os.path.join(raw_url_dir, f\"covid19_url_cleaned_{date_tag}.txt\")\n",
    "\n",
    "    if not os.path.exists(url_file):\n",
    "        print(f\"‚ö†Ô∏è URL file missing for {date_tag}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {date_tag} ...\")\n",
    "\n",
    "    with open(content_file, encoding=\"utf-8\") as f:\n",
    "        contents = [line.strip() for line in f]\n",
    "\n",
    "    with open(url_file, encoding=\"utf-8\") as f:\n",
    "        urls = [line.strip() for line in f]\n",
    "\n",
    "    assert len(contents) == len(urls), f\"Line count mismatch in {date_tag}\"\n",
    "\n",
    "    unique_contents = []\n",
    "    unique_urls = []\n",
    "\n",
    "    for content, url in zip(contents, urls):\n",
    "        if content not in seen_contents:\n",
    "            seen_contents.add(content)\n",
    "            unique_contents.append(content)\n",
    "            unique_urls.append(url)\n",
    "\n",
    "    # Save deduplicated version for this date\n",
    "    out_content = os.path.join(cleaned_content_dir, f\"covid19_content_deduped_{date_tag}.txt\")\n",
    "    out_url = os.path.join(cleaned_url_dir, f\"covid19_url_deduped_{date_tag}.txt\")\n",
    "\n",
    "    with open(out_content, \"w\", encoding=\"utf-8\") as f:\n",
    "        for c in unique_contents:\n",
    "            f.write(c + \"\\n\")\n",
    "\n",
    "    with open(out_url, \"w\", encoding=\"utf-8\") as f:\n",
    "        for u in unique_urls:\n",
    "            f.write(u + \"\\n\")\n",
    "\n",
    "    list_unique_pages.append(len(unique_contents))\n",
    "    list_date_tag.append(date_tag)\n",
    "\n",
    "    print(f\"‚úÖ {date_tag}: kept {len(unique_contents)} unique pages (out of {len(contents)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec97c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a dataframe\n",
    "df_unique_pages = pd.DataFrame({\"date_tag\": list_date_tag, \"unique_pages\": list_unique_pages})\n",
    "df_unique_pages[\"date_tag\"] = pd.to_datetime(df_unique_pages[\"date_tag\"])\n",
    "df_unique_pages.set_index(\"date_tag\", inplace=True)\n",
    "df_unique_pages.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194a192",
   "metadata": {},
   "source": [
    "## 4. Advanced analysis with embeddings and semantic search\n",
    "\n",
    "Now, let's use embeddings to perform semantic search and analysis on our corpus. \n",
    "\n",
    "We will use the corpus and the corresponding urls preprocessed in the previous notebook (exp-02_Exploring_NLNZ_WebArchive.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f7517",
   "metadata": {},
   "source": [
    "First, let's import the required packages used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cfd0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "\n",
    "# define the collection name\n",
    "db_collection_name = \"covid_webpages\"\n",
    "\n",
    "# define embedding model\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# define chroma db path\n",
    "path_chroma = os.path.join(res_folder, \"chroma_db\")\n",
    "\n",
    "# define the input directories\n",
    "input_content_dir = cleaned_content_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf284c87",
   "metadata": {},
   "source": [
    "Here we will use Chroma DB to store and query the embeddings. Chroma DB is a vector database that allows us to store and query embeddings efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=path_chroma)\n",
    "collection = client.get_or_create_collection(name=db_collection_name)\n",
    "\n",
    "model = SentenceTransformer(embedding_model)\n",
    "\n",
    "files = sorted([f for f in os.listdir(input_content_dir) if f.endswith(\".txt\")])\n",
    "\n",
    "for fname in files:\n",
    "    content_file_path = os.path.join(input_content_dir, fname)\n",
    "    with open(content_file_path, encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    \n",
    "    url_file_path = content_file_path.replace(\"content\", \"url\")\n",
    "    with open(url_file_path, encoding=\"utf-8\") as f:\n",
    "        urls = [line.strip() for line in f]\n",
    "    \n",
    "    embeddings = model.encode(lines, show_progress_bar=True, convert_to_numpy=True)\n",
    "    # we can also try to encode the urls since they also contain some useful information\n",
    "    # urls_updated = [\" \".join(url.split(\"https://covid19.govt.nz\")[1].split(\"/\")).strip() for url in urls]\n",
    "    # embeddings = model.encode(urls_updated, show_progress_bar=True, convert_to_numpy=True)\n",
    "    datestr = extract_date(fname)\n",
    "    \n",
    "    # Add to Chroma\n",
    "    try:\n",
    "        collection.add(\n",
    "            ids=[f\"{fname}_{i}\" for i in range(len(lines))],\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=lines,\n",
    "            metadatas=[{\"filename\": fname, \"date\": datestr, \"line\": i, \"url\": url} for i, url in enumerate(urls)]\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(f\"‚ö†Ô∏è Skipped {fname} due to ValueError\")\n",
    "\n",
    "print(\"‚úÖ Indexed all lines into Chroma!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f805dd",
   "metadata": {},
   "source": [
    "Having the vector database ready, we can now apply a semantic search to find the most relevant documents for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, n_results=5):\n",
    "    model = SentenceTransformer(embedding_model)\n",
    "\n",
    "    client = chromadb.PersistentClient(path=path_chroma)\n",
    "    collection = client.get_or_create_collection(name=db_collection_name)\n",
    "\n",
    "    query_emb = model.encode([query], convert_to_numpy=True).tolist()\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_emb,\n",
    "        n_results=5\n",
    "    )\n",
    "\n",
    "    for text, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        print(f\"{meta['filename']} (line {meta['line']}) ‚Äî {meta['url']}\")\n",
    "        print(f\"‚Üí {text[:]}...\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = semantic_search(\"What is the impact of the pandemic on the economy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a0965",
   "metadata": {},
   "source": [
    "## 5. Building a question-answering system with the corpus\n",
    "\n",
    "Finally, let's build a simple question-answering system using our corpus and a language model. For this section, we'll need to install additional packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages for the QA system\n",
    "!pip -q install transformers langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5351f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b00d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the same embedding model you used to create Chroma\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the persistent Chroma DB\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=path_chroma,\n",
    "    collection_name=db_collection_name,\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "# Build a retriever from the vectorstore directly (no need to import VectorStoreRetriever)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b7d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# qa_pipeline = pipeline(\n",
    "#     \"question-answering\",\n",
    "#     model=\"distilbert-base-cased-distilled-squad\",\n",
    "#     tokenizer=\"distilbert-base-cased-distilled-squad\"\n",
    "# )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
    "\n",
    "# Use a small open-source text generation model\n",
    "gen_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    tokenizer=\"google/flan-t5-base\",\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=gen_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663bea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" simply concatenates retrieved docs\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd08d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the impact of the pandemic on the economy?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"üîπ Question:\", query)\n",
    "print(\"üîπ Answer:\", result[\"result\"])\n",
    "print(\"\\nüîπ Sources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata.get(\"filename\"), \"‚Üí\", doc.page_content[:200], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2626d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to work with text files extracted from web archive HTML snapshots. We covered:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Extracting text from web archive HTML\n",
    "3. Cleaning and preprocessing the text data\n",
    "4. Advanced analysis with embeddings and semantic search\n",
    "5. Building a question-answering system with the corpus\n",
    "\n",
    "These techniques can be applied to any web archive corpus to extract insights and make the data more accessible and useful for research and analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
