{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c782d34d",
   "metadata": {},
   "source": [
    "# Processing Web Archive Text Corpus\n",
    "\n",
    "This notebook demonstrates how to work with text files extracted from web archive HTML snapshots. We'll cover the following steps:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Extracting text from web archive HTML\n",
    "3. Cleaning and preprocessing the text data\n",
    "4. Advanced analysis with embeddings and semantic search\n",
    "5. Building a question-answering system with the corpus\n",
    "\n",
    "**Purpose**: This notebook shows how to process and analyze text extracted from web archives, enabling researchers to gain insights from historical web content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a36034",
   "metadata": {},
   "source": [
    "## 1. Setting up the environment\n",
    "\n",
    "First, let's install the required packages for web archive processing, text analysis, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b53589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core web archive processing packages\n",
    "!pip -q install warcio>=1.7.4 validators boto3>=1.40.26 s3fs bs4 wordcloud\n",
    "\n",
    "# Install packages for web screenshots (optional)\n",
    "!pip -q install selenium chromedriver-autoinstaller\n",
    "\n",
    "# Install packages for embeddings and semantic search\n",
    "!pip -q install sentence-transformers chromadb\n",
    "\n",
    "# Install the NLNZ Web Archive Toolkit\n",
    "!pip -q install -i https://test.pypi.org/simple/ wa-nlnz-toolkit==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment and set appropriate paths\n",
    "# This allows the notebook to run in different environments (local or Colab)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Set default result folder based on environment\n",
    "if IN_COLAB:\n",
    "    res_folder = \"/content/sample_data\"\n",
    "else:\n",
    "    res_folder = \"./sample_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b856e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wa_nlnz_toolkit as want\n",
    "from tqdm import tqdm  # Progress bar for long-running operations\n",
    "from glob import glob  # File pattern matching\n",
    "from collections import Counter  # For counting word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd67cfa",
   "metadata": {},
   "source": [
    "## 2. Extracting text from web archive HTML\n",
    "\n",
    "Web archives store content in WARC (Web ARChive) files. Here we demonstrate how to extract HTML content from these files and convert it to plain text using the `wa_nlnz_toolkit` library.\n",
    "\n",
    "The process involves two main steps:\n",
    "1. Extract the HTML payload from a WARC file using `extract_payload`\n",
    "2. Parse the HTML to extract meaningful text using `extract_content_html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract HTML payload from a WARC file\n",
    "# This demonstrates accessing a specific web page snapshot from an archive\n",
    "warc_file = \"s3://ndha-public-data-ap-southeast-2/iPRES-2025/sample-data/covid19.govt.nz/2023-12-14_IE89493927/FL89493929_NLNZ-20231212233435565-00000-72544~wlgprdwctweb01.natlib.govt.nz~8443.warc.gz\"\n",
    "offset = 3126252  # Byte offset where the record starts in the WARC file\n",
    "\n",
    "# Extract the HTML payload\n",
    "html_payload = want.extract_payload(warc_file, offset)\n",
    "\n",
    "# Check if extraction was successful\n",
    "if html_payload:\n",
    "    print(f\"Successfully extracted HTML payload of {len(html_payload)} bytes\")\n",
    "else:\n",
    "    print(\"Failed to extract HTML payload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text content from the HTML payload\n",
    "# This converts raw HTML into readable text paragraphs\n",
    "if html_payload:\n",
    "    # Use the toolkit's function to extract content\n",
    "    paragraphs = want.extract_content_html(html_payload)\n",
    "    \n",
    "    # Print the first few paragraphs as a sample\n",
    "    print(f\"Extracted {len(paragraphs)} paragraphs. Here are the first 5:\")\n",
    "    for i, para in enumerate(paragraphs[:5]):\n",
    "        print(f\"\\n{i+1}. {para}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225945f9",
   "metadata": {},
   "source": [
    "## 3. Cleaning and preprocessing the text data\n",
    "\n",
    "Raw text extracted from web archives often contains duplicates, irrelevant content, and formatting issues. This section demonstrates how to clean and preprocess this data to make it more suitable for analysis.\n",
    "\n",
    "The main preprocessing steps include:\n",
    "1. Loading raw text files\n",
    "2. Removing duplicates across snapshots\n",
    "3. Maintaining the relationship between content and source URLs\n",
    "4. Saving the cleaned data for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc69663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for raw and cleaned data\n",
    "raw_folder_path = os.path.join(res_folder, \"covid19_corpus/raw/\")\n",
    "cleaned_folder_path = os.path.join(res_folder, \"covid19_corpus/cleaned/\")\n",
    "\n",
    "# Create directory structure for cleaned data\n",
    "# We maintain separate directories for content and URLs\n",
    "cleaned_content_dir = os.path.join(cleaned_folder_path, \"content\")\n",
    "cleaned_url_dir = os.path.join(cleaned_folder_path, \"url\")\n",
    "os.makedirs(cleaned_content_dir, exist_ok=True)\n",
    "os.makedirs(cleaned_url_dir, exist_ok=True)\n",
    "\n",
    "# List all text files in the raw folder\n",
    "raw_content_dir = os.path.join(raw_folder_path, \"content\")\n",
    "raw_url_dir = os.path.join(raw_folder_path, \"url\")\n",
    "raw_content_files = [f for f in os.listdir(raw_content_dir) if f.endswith(\".txt\")]\n",
    "print(f\"Found {len(raw_content_files)} raw text files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ff77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract date from filename\n",
    "def extract_date(fname):\n",
    "    \"\"\"Extract date in YYYY-MM-DD format from a filename.\n",
    "    \n",
    "    Args:\n",
    "        fname: Filename string containing a date\n",
    "        \n",
    "    Returns:\n",
    "        Date string in YYYY-MM-DD format or None if not found\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', fname)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "# Process all content files chronologically\n",
    "content_files = sorted(glob(os.path.join(raw_content_dir, \"covid19_content_cleaned_*.txt\")))\n",
    "seen_contents = set()  # Track unique content across all snapshots\n",
    "\n",
    "# Lists to store data for later analysis\n",
    "list_date_tag = []\n",
    "list_unique_pages = []\n",
    "\n",
    "# Process each content file and its corresponding URL file\n",
    "for content_file in content_files:\n",
    "    # Extract date from filename\n",
    "    date_tag = extract_date(os.path.basename(content_file))\n",
    "    url_file = os.path.join(raw_url_dir, f\"covid19_url_cleaned_{date_tag}.txt\")\n",
    "\n",
    "    # Skip if URL file is missing\n",
    "    if not os.path.exists(url_file):\n",
    "        print(f\"‚ö†Ô∏è URL file missing for {date_tag}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {date_tag} ...\")\n",
    "\n",
    "    # Read content and URL files\n",
    "    with open(content_file, encoding=\"utf-8\") as f:\n",
    "        contents = [line.strip() for line in f]\n",
    "\n",
    "    with open(url_file, encoding=\"utf-8\") as f:\n",
    "        urls = [line.strip() for line in f]\n",
    "\n",
    "    # Verify content and URL files have matching line counts\n",
    "    assert len(contents) == len(urls), f\"Line count mismatch in {date_tag}\"\n",
    "\n",
    "    # Filter out duplicates while preserving content-URL relationship\n",
    "    unique_contents = []\n",
    "    unique_urls = []\n",
    "\n",
    "    for content, url in zip(contents, urls):\n",
    "        if content not in seen_contents:\n",
    "            seen_contents.add(content)\n",
    "            unique_contents.append(content)\n",
    "            unique_urls.append(url)\n",
    "\n",
    "    # Save deduplicated content and URLs\n",
    "    out_content = os.path.join(cleaned_content_dir, f\"covid19_content_deduped_{date_tag}.txt\")\n",
    "    out_url = os.path.join(cleaned_url_dir, f\"covid19_url_deduped_{date_tag}.txt\")\n",
    "\n",
    "    with open(out_content, \"w\", encoding=\"utf-8\") as f:\n",
    "        for c in unique_contents:\n",
    "            f.write(c + \"\\n\")\n",
    "\n",
    "    with open(out_url, \"w\", encoding=\"utf-8\") as f:\n",
    "        for u in unique_urls:\n",
    "            f.write(u + \"\\n\")\n",
    "\n",
    "    # Store metrics for analysis\n",
    "    list_unique_pages.append(len(unique_contents))\n",
    "    list_date_tag.append(date_tag)\n",
    "\n",
    "    print(f\"‚úÖ {date_tag}: kept {len(unique_contents)} unique pages (out of {len(contents)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec97c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the number of unique pages over time\n",
    "# This helps understand how the website content evolved\n",
    "df_unique_pages = pd.DataFrame({\"date_tag\": list_date_tag, \"unique_pages\": list_unique_pages})\n",
    "df_unique_pages[\"date_tag\"] = pd.to_datetime(df_unique_pages[\"date_tag\"])\n",
    "df_unique_pages.set_index(\"date_tag\", inplace=True)\n",
    "\n",
    "# Create a time series plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = df_unique_pages.plot()\n",
    "ax.set_title(\"Number of New Unique Pages Over Time\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194a192",
   "metadata": {},
   "source": [
    "## 4. Advanced analysis with embeddings and semantic search\n",
    "\n",
    "Traditional keyword search is limited when analyzing large text corpora. Semantic search using embeddings allows us to find content based on meaning rather than exact word matches.\n",
    "\n",
    "In this section, we'll:\n",
    "1. Create vector embeddings for each text snippet\n",
    "2. Store these embeddings in a vector database (ChromaDB)\n",
    "3. Implement semantic search to find relevant content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f7517",
   "metadata": {},
   "source": [
    "First, let's import the required packages for embedding and vector storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cfd0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import embedding and vector database libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Configuration for vector database\n",
    "db_collection_name = \"covid_webpages\"  # Name for our vector collection\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"  # Pre-trained embedding model\n",
    "path_chroma = os.path.join(res_folder, \"chroma_db\")  # Path to store the vector database\n",
    "input_content_dir = cleaned_content_dir  # Directory with cleaned text files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf284c87",
   "metadata": {},
   "source": [
    "ChromaDB is a vector database designed for efficient storage and retrieval of embeddings. It allows us to perform similarity searches across our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client and collection\n",
    "client = chromadb.PersistentClient(path=path_chroma)\n",
    "collection = client.get_or_create_collection(name=db_collection_name)\n",
    "\n",
    "# Load the sentence transformer model for creating embeddings\n",
    "model = SentenceTransformer(embedding_model)\n",
    "\n",
    "# Get all cleaned text files\n",
    "files = sorted([f for f in os.listdir(input_content_dir) if f.endswith(\".txt\")])\n",
    "\n",
    "# Process each file and add its contents to the vector database\n",
    "for fname in files:\n",
    "    # Load content file\n",
    "    content_file_path = os.path.join(input_content_dir, fname)\n",
    "    with open(content_file_path, encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    \n",
    "    # Load corresponding URL file\n",
    "    url_file_path = content_file_path.replace(\"content\", \"url\")\n",
    "    with open(url_file_path, encoding=\"utf-8\") as f:\n",
    "        urls = [line.strip() for line in f]\n",
    "    \n",
    "    # Create embeddings for each line of text\n",
    "    # This converts text into numerical vectors that capture semantic meaning\n",
    "    embeddings = model.encode(lines, show_progress_bar=True, convert_to_numpy=True)\n",
    "    datestr = extract_date(fname)\n",
    "    \n",
    "    # Add embeddings and metadata to ChromaDB\n",
    "    try:\n",
    "        collection.add(\n",
    "            ids=[f\"{fname}_{i}\" for i in range(len(lines))],\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=lines,\n",
    "            metadatas=[{\"filename\": fname, \"date\": datestr, \"line\": i, \"url\": url} for i, url in enumerate(urls)]\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(f\"‚ö†Ô∏è Skipped {fname} due to ValueError\")\n",
    "\n",
    "print(\"‚úÖ Indexed all lines into Chroma!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f805dd",
   "metadata": {},
   "source": [
    "Now that we have our vector database ready, we can perform semantic searches to find content based on meaning rather than exact keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, n_results=5):\n",
    "    \"\"\"Perform semantic search on the corpus using vector embeddings.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query as a string\n",
    "        n_results: Number of results to return (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing search results from ChromaDB\n",
    "    \"\"\"\n",
    "    # Load the embedding model\n",
    "    model = SentenceTransformer(embedding_model)\n",
    "\n",
    "    # Connect to the vector database\n",
    "    client = chromadb.PersistentClient(path=path_chroma)\n",
    "    collection = client.get_or_create_collection(name=db_collection_name)\n",
    "\n",
    "    # Convert query to embedding vector\n",
    "    query_emb = model.encode([query], convert_to_numpy=True).tolist()\n",
    "\n",
    "    # Search for similar content in the database\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_emb,\n",
    "        n_results=n_results\n",
    "    )\n",
    "\n",
    "    # Display results with metadata\n",
    "    for text, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        print(f\"{meta['filename']} (line {meta['line']}) ‚Äî {meta['url']}\")\n",
    "        print(f\"‚Üí {text[:]}\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example semantic search query\n",
    "# This demonstrates finding content related to economic impacts without requiring exact keyword matches\n",
    "res = semantic_search(\"What is the impact of the pandemic on the economy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a0965",
   "metadata": {},
   "source": [
    "## 5. Building a question-answering system with the corpus\n",
    "\n",
    "Beyond search, we can build a question-answering system that combines our vector database with language models to provide direct answers to questions about the corpus.\n",
    "\n",
    "This approach, known as Retrieval-Augmented Generation (RAG), involves:\n",
    "1. Retrieving relevant passages using semantic search\n",
    "2. Using a language model to generate answers based on the retrieved content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages for the QA system\n",
    "!pip -q install transformers langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5351f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for building the QA system\n",
    "from transformers import pipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b00d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the embedding model for retrieval\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Connect to the existing vector database\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=path_chroma,\n",
    "    collection_name=db_collection_name,\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "# Create a retriever that will fetch relevant documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})  # Retrieve 5 most relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b7d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a language model for answering questions\n",
    "# We use a small open-source text generation model (flan-t5-base)\n",
    "gen_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    tokenizer=\"google/flan-t5-base\",\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Wrap the pipeline in a LangChain compatible format\n",
    "llm = HuggingFacePipeline(pipeline=gen_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663bea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a question-answering chain\n",
    "# This combines retrieval and generation into a single pipeline\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" concatenates retrieved docs into the prompt\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True  # Include source documents in the response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd08d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example question to demonstrate the QA system\n",
    "query = \"What is the impact of the pandemic on the economy?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# Display the question, answer, and sources\n",
    "print(\"üîπ Question:\", query)\n",
    "print(\"üîπ Answer:\", result[\"result\"])\n",
    "print(\"\\nüîπ Sources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata.get(\"filename\"), \"‚Üí\", doc.page_content[:200], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2626d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated a complete workflow for processing and analyzing text extracted from web archives:\n",
    "\n",
    "1. **Extraction**: Converting web archive HTML to plain text\n",
    "2. **Preprocessing**: Cleaning and deduplicating text data\n",
    "3. **Semantic Search**: Finding content based on meaning using vector embeddings\n",
    "4. **Question Answering**: Building an AI system that can answer questions about the corpus\n",
    "\n",
    "These techniques enable researchers to extract valuable insights from web archives, making historical web content more accessible and useful for analysis.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different embedding models for improved search quality\n",
    "- Apply topic modeling to discover themes in the corpus\n",
    "- Integrate with larger language models for more sophisticated question answering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
